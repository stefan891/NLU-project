{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nBtlz2TBSACn",
        "outputId": "cc8e44d5-38fb-4dd3-bb11-424c36db7b05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nBtlz2TBSACn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/My Drive/My Folder\""
      ],
      "metadata": {
        "id": "Jk8jOkZ_T3LN"
      },
      "id": "Jk8jOkZ_T3LN",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "008934a4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "008934a4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# RNN Elman version\n",
        "# We are not going to use this since for efficiently purposes it's better to use the RNN layer provided by pytorch\n",
        "\n",
        "class RNN_cell(nn.Module):\n",
        "    def __init__(self,  hidden_size, input_size, output_size, vocab_size, dropout=0.1):\n",
        "        super(RNN_cell, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.U = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, vocab_size)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, prev_hidden, word):\n",
        "        input_emb = self.W(word)\n",
        "        prev_hidden_rep = self.U(prev_hidden)\n",
        "        # ht = σ(Wx + Uht-1 + b)\n",
        "        hidden_state = self.sigmoid(input_emb + prev_hidden_rep)\n",
        "        # yt = σ(Vht + b)\n",
        "        output = self.output(hidden_state)\n",
        "        return hidden_state, output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LM_RNN(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
        "                 emb_dropout=0.1, n_layers=1):\n",
        "        super(LM_RNN, self).__init__()\n",
        "        # Token ids to vectors, we will better see this in the next lab\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "        #create a dictionary that associate words with meaning\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n",
        "        self.pad_token = pad_index\n",
        "        # Linear layer to project the hidden layer to our output space\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        emb = self.embedding(input_sequence)\n",
        "        rnn_out, _  = self.rnn(emb)\n",
        "        output = self.output(rnn_out).permute(0,2,1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "L-gzZR0qv-jt"
      },
      "id": "L-gzZR0qv-jt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda:0'"
      ],
      "metadata": {
        "id": "lgpEORLDwPWb"
      },
      "id": "lgpEORLDwPWb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the corpus\n",
        "\n",
        "def read_file(path, eos_token=\"<eos>\"):\n",
        "    output = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            output.append(line.strip() + \" \" + eos_token)\n",
        "    return output\n",
        "\n",
        "# Vocab with tokens to ids\n",
        "def get_vocab(corpus, special_tokens=[]):\n",
        "    output = {}\n",
        "    i = 0\n",
        "    for st in special_tokens:\n",
        "        output[st] = i\n",
        "        i += 1\n",
        "    for sentence in corpus:\n",
        "        for w in sentence.split():\n",
        "            if w not in output:\n",
        "                output[w] = i\n",
        "                i += 1\n",
        "    return output"
      ],
      "metadata": {
        "id": "SEcooElSwSAG"
      },
      "id": "SEcooElSwSAG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If you are using Colab, run these lines\n",
        "!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n",
        "!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n",
        "!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt"
      ],
      "metadata": {
        "id": "dgjzlMKiwU1O",
        "outputId": "caa71691-ce84-4398-ca1a-728a5f217165",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dgjzlMKiwU1O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-24 11:46:30--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 449945 (439K) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.test.txt’\n",
            "\n",
            "\rptb.test.txt          0%[                    ]       0  --.-KB/s               \rptb.test.txt        100%[===================>] 439.40K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-06-24 11:46:30 (17.2 MB/s) - ‘dataset/PennTreeBank/ptb.test.txt’ saved [449945/449945]\n",
            "\n",
            "--2025-06-24 11:46:30--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 399782 (390K) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.valid.txt’\n",
            "\n",
            "ptb.valid.txt       100%[===================>] 390.41K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-06-24 11:46:30 (12.5 MB/s) - ‘dataset/PennTreeBank/ptb.valid.txt’ saved [399782/399782]\n",
            "\n",
            "--2025-06-24 11:46:30--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5101618 (4.9M) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.train.txt’\n",
            "\n",
            "ptb.train.txt       100%[===================>]   4.87M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-06-24 11:46:31 (93.4 MB/s) - ‘dataset/PennTreeBank/ptb.train.txt’ saved [5101618/5101618]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw = read_file(\"dataset/PennTreeBank/ptb.train.txt\")\n",
        "dev_raw = read_file(\"dataset/PennTreeBank/ptb.valid.txt\")\n",
        "test_raw = read_file(\"dataset/PennTreeBank/ptb.test.txt\")"
      ],
      "metadata": {
        "id": "gwDPEKhzwuj4"
      },
      "id": "gwDPEKhzwuj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"
      ],
      "metadata": {
        "id": "FgAk18YYw27-"
      },
      "id": "FgAk18YYw27-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "M2GS4OkCw5ON",
        "outputId": "de5ba6d9-4d22-4fd6-9114-bf12153abeb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "M2GS4OkCw5ON",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This class computes and stores our vocab\n",
        "# Word to ids and ids to word\n",
        "class Lang():\n",
        "    def __init__(self, corpus, special_tokens=[]):\n",
        "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
        "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
        "    def get_vocab(self, corpus, special_tokens=[]):\n",
        "        output = {}\n",
        "        i = 0\n",
        "        for st in special_tokens:\n",
        "            output[st] = i\n",
        "            i += 1\n",
        "        for sentence in corpus:\n",
        "            for w in sentence.split():\n",
        "                if w not in output:\n",
        "                    output[w] = i\n",
        "                    i += 1\n",
        "        return output"
      ],
      "metadata": {
        "id": "DGUP_Es0w8NF"
      },
      "id": "DGUP_Es0w8NF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
      ],
      "metadata": {
        "id": "izzf7p6iw-jk"
      },
      "id": "izzf7p6iw-jk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "class PennTreeBank (data.Dataset):\n",
        "    # Mandatory methods are __init__, __len__ and __getitem__\n",
        "    def __init__(self, corpus, lang):\n",
        "        self.source = []\n",
        "        self.target = []\n",
        "\n",
        "        for sentence in corpus:\n",
        "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
        "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
        "            # See example in section 6.2\n",
        "\n",
        "        self.source_ids = self.mapping_seq(self.source, lang)\n",
        "        self.target_ids = self.mapping_seq(self.target, lang)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src= torch.LongTensor(self.source_ids[idx])\n",
        "        trg = torch.LongTensor(self.target_ids[idx])\n",
        "        sample = {'source': src, 'target': trg}\n",
        "        return sample\n",
        "\n",
        "    # Auxiliary methods\n",
        "\n",
        "    def mapping_seq(self, data, lang): # Map sequences of tokens to corresponding computed in Lang class\n",
        "        res = []\n",
        "        for seq in data:\n",
        "            tmp_seq = []\n",
        "            for x in seq:\n",
        "                if x in lang.word2id:\n",
        "                    tmp_seq.append(lang.word2id[x])\n",
        "                else:\n",
        "                    print('OOV found!')\n",
        "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
        "                    break\n",
        "            res.append(tmp_seq)\n",
        "        return res"
      ],
      "metadata": {
        "id": "OSzFJN3VxB-g"
      },
      "id": "OSzFJN3VxB-g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PennTreeBank(train_raw, lang)\n",
        "dev_dataset = PennTreeBank(dev_raw, lang)\n",
        "test_dataset = PennTreeBank(test_raw, lang)"
      ],
      "metadata": {
        "id": "gxs9dq8UxEiR"
      },
      "id": "gxs9dq8UxEiR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(data, pad_token):\n",
        "    def merge(sequences):\n",
        "        '''\n",
        "        merge from batch * sent_len to batch * max_len\n",
        "        '''\n",
        "        lengths = [len(seq) for seq in sequences]\n",
        "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
        "        # Pad token is zero in our case\n",
        "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape\n",
        "        # batch_size X maximum length of a sequence\n",
        "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
        "        for i, seq in enumerate(sequences):\n",
        "            end = lengths[i]\n",
        "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
        "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
        "        return padded_seqs, lengths\n",
        "\n",
        "    # Sort data by seq lengths\n",
        "\n",
        "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True)\n",
        "    new_item = {}\n",
        "    for key in data[0].keys():\n",
        "        new_item[key] = [d[key] for d in data]\n",
        "\n",
        "    source, _ = merge(new_item[\"source\"])\n",
        "    target, lengths = merge(new_item[\"target\"])\n",
        "\n",
        "    new_item[\"source\"] = source.to(DEVICE)\n",
        "    new_item[\"target\"] = target.to(DEVICE)\n",
        "    new_item[\"number_tokens\"] = sum(lengths)\n",
        "    return new_item\n",
        "\n",
        "# Dataloader instantiation\n",
        "# You can reduce the batch_size if the GPU memory is not enough\n",
        "# take inputs from our data, we have to define the way\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
      ],
      "metadata": {
        "id": "QHWwNN29xJDt"
      },
      "id": "QHWwNN29xJDt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def train_loop(data, optimizer, criterion, model, clip=5):\n",
        "    model.train()\n",
        "    loss_array = []\n",
        "    number_of_tokens = []\n",
        "\n",
        "    for sample in data:\n",
        "        optimizer.zero_grad() # Zeroing the gradient\n",
        "        output = model(sample['source'])\n",
        "        loss = criterion(output, sample['target'])\n",
        "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
        "        number_of_tokens.append(sample[\"number_tokens\"])\n",
        "        loss.backward() # Compute the gradient, deleting the computational graph\n",
        "        # clip the gradient to avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step() # Update the weights\n",
        "\n",
        "    return sum(loss_array)/sum(number_of_tokens)\n",
        "\n",
        "def eval_loop(data, eval_criterion, model):\n",
        "    model.eval()\n",
        "    loss_to_return = []\n",
        "    loss_array = []\n",
        "    number_of_tokens = []\n",
        "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
        "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
        "        for sample in data:\n",
        "            output = model(sample['source'])\n",
        "            loss = eval_criterion(output, sample['target'])\n",
        "            loss_array.append(loss.item())\n",
        "            number_of_tokens.append(sample[\"number_tokens\"])\n",
        "\n",
        "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
        "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
        "    return ppl, loss_to_return\n",
        "\n",
        "def init_weights(mat):\n",
        "    for m in mat.modules():\n",
        "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight_ih' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'weight_hh' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'bias' in name:\n",
        "                    param.data.fill_(0)\n",
        "        else:\n",
        "            if type(m) in [nn.Linear]:\n",
        "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
        "                if m.bias != None:\n",
        "                    m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "i3faWQhxxMgD"
      },
      "id": "i3faWQhxxMgD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# Experiment also with a smaller or bigger model by changing hid and emb sizes\n",
        "# A large model tends to overfit\n",
        "hid_size = 200\n",
        "emb_size = 300\n",
        "\n",
        "# Don't forget to experiment with a lower training batch size\n",
        "# Increasing the back propagation steps can be seen as a regularization step\n",
        "\n",
        "# With SGD try with an higher learning rate (> 1 for instance)\n",
        "lr = 0.1 # This is definitely not good for SGD\n",
        "clip = 5 # Clip the gradient\n",
        "\n",
        "vocab_len = len(lang.word2id)\n",
        "\n",
        "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(DEVICE)\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
        "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
      ],
      "metadata": {
        "id": "y9eXbrNuxNj5"
      },
      "id": "y9eXbrNuxNj5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "n_epochs = 100\n",
        "patience = 3\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "best_ppl = math.inf\n",
        "best_model = None\n",
        "pbar = tqdm(range(1,n_epochs))\n",
        "#If the PPL is too high try to change the learning rate\n",
        "for epoch in pbar:\n",
        "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)\n",
        "    if epoch % 1 == 0:\n",
        "        sampled_epochs.append(epoch)\n",
        "        losses_train.append(np.asarray(loss).mean())\n",
        "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
        "        losses_dev.append(np.asarray(loss_dev).mean())\n",
        "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
        "        if  ppl_dev < best_ppl: # the lower, the better\n",
        "            best_ppl = ppl_dev\n",
        "            best_model = copy.deepcopy(model).to('cpu')\n",
        "            patience = 3\n",
        "        else:\n",
        "            patience -= 1\n",
        "\n",
        "        if patience <= 0: # Early stopping with patience\n",
        "            break # Not nice but it keeps the code clean\n",
        "\n",
        "best_model.to(DEVICE)\n",
        "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)\n",
        "print('Test ppl: ', final_ppl)"
      ],
      "metadata": {
        "id": "W_8y07jIxTcH",
        "outputId": "f37908c1-5118-4ff5-d35b-be59d4488cc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "W_8y07jIxTcH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PPL: 7506.436635: 100%|██████████| 99/99 [31:55<00:00, 19.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test ppl:  7447.126973140212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To save the model\n",
        "path = '/content/drive/MyDrive/model_bin/RNN_lr_0.1.pt'\n",
        "torch.save(model.state_dict(), path)\n",
        "#To load the model you need to initialize it\n",
        "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(DEVICE)\n",
        "#Then you load it\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "rs7Sw1C4D5bt",
        "outputId": "cc2aed3a-3d93-449f-c570-bcccb5a4d521",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rs7Sw1C4D5bt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}