{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nBtlz2TBSACn",
        "outputId": "bc91d41a-f5ae-4e6c-845a-ea8b5dbab519",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nBtlz2TBSACn",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p \"/content/drive/My Drive/My Folder\""
      ],
      "metadata": {
        "id": "Jk8jOkZ_T3LN"
      },
      "id": "Jk8jOkZ_T3LN",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "008934a4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "008934a4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LM_RNN(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
        "                 emb_dropout=0.1, n_layers=1):\n",
        "        super(LM_RNN, self).__init__()\n",
        "        # Token ids to vectors, we will better see this in the next lab\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "        #create a dictionary that associate words with meaning\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n",
        "        self.pad_token = pad_index\n",
        "        # Linear layer to project the hidden layer to our output space\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        emb = self.embedding(input_sequence)\n",
        "        rnn_out, _  = self.rnn(emb)\n",
        "        output = self.output(rnn_out).permute(0,2,1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "L-gzZR0qv-jt"
      },
      "id": "L-gzZR0qv-jt",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LM_LSTM(nn.Module):\n",
        "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
        "                 emb_dropout=0.1, n_layers=1):\n",
        "        super(LM_LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
        "        self.lstm = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)\n",
        "        self.pad_token = pad_index\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        emb = self.embedding(input_sequence)\n",
        "        lstm_out, _  = self.lstm(emb)\n",
        "        output = self.output(lstm_out).permute(0,2,1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "c-NFtjo-ebI9"
      },
      "id": "c-NFtjo-ebI9",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda:0'"
      ],
      "metadata": {
        "id": "lgpEORLDwPWb"
      },
      "id": "lgpEORLDwPWb",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the corpus\n",
        "\n",
        "def read_file(path, eos_token=\"<eos>\"):\n",
        "    output = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            output.append(line.strip() + \" \" + eos_token)\n",
        "    return output\n",
        "\n",
        "# Vocab with tokens to ids\n",
        "def get_vocab(corpus, special_tokens=[]):\n",
        "    output = {}\n",
        "    i = 0\n",
        "    for st in special_tokens:\n",
        "        output[st] = i\n",
        "        i += 1\n",
        "    for sentence in corpus:\n",
        "        for w in sentence.split():\n",
        "            if w not in output:\n",
        "                output[w] = i\n",
        "                i += 1\n",
        "    return output"
      ],
      "metadata": {
        "id": "SEcooElSwSAG"
      },
      "id": "SEcooElSwSAG",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If you are using Colab, run these lines\n",
        "!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n",
        "!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n",
        "!wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt"
      ],
      "metadata": {
        "id": "dgjzlMKiwU1O",
        "outputId": "db8ecfcc-7fb7-4709-bbed-c680d5452c78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dgjzlMKiwU1O",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-27 15:01:00--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 449945 (439K) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.test.txt’\n",
            "\n",
            "\rptb.test.txt          0%[                    ]       0  --.-KB/s               \rptb.test.txt        100%[===================>] 439.40K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-06-27 15:01:01 (13.3 MB/s) - ‘dataset/PennTreeBank/ptb.test.txt’ saved [449945/449945]\n",
            "\n",
            "--2025-06-27 15:01:01--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 399782 (390K) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.valid.txt’\n",
            "\n",
            "ptb.valid.txt       100%[===================>] 390.41K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-06-27 15:01:01 (10.9 MB/s) - ‘dataset/PennTreeBank/ptb.valid.txt’ saved [399782/399782]\n",
            "\n",
            "--2025-06-27 15:01:01--  https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5101618 (4.9M) [text/plain]\n",
            "Saving to: ‘dataset/PennTreeBank/ptb.train.txt’\n",
            "\n",
            "ptb.train.txt       100%[===================>]   4.87M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-06-27 15:01:01 (79.8 MB/s) - ‘dataset/PennTreeBank/ptb.train.txt’ saved [5101618/5101618]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw = read_file(\"dataset/PennTreeBank/ptb.train.txt\")\n",
        "dev_raw = read_file(\"dataset/PennTreeBank/ptb.valid.txt\")\n",
        "test_raw = read_file(\"dataset/PennTreeBank/ptb.test.txt\")"
      ],
      "metadata": {
        "id": "gwDPEKhzwuj4"
      },
      "id": "gwDPEKhzwuj4",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"
      ],
      "metadata": {
        "id": "FgAk18YYw27-"
      },
      "id": "FgAk18YYw27-",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "id": "M2GS4OkCw5ON",
        "outputId": "71fd1e32-52c9-4f0b-bdcc-8b67940fd39a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "M2GS4OkCw5ON",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This class computes and stores our vocab\n",
        "# Word to ids and ids to word\n",
        "class Lang():\n",
        "    def __init__(self, corpus, special_tokens=[]):\n",
        "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
        "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
        "    def get_vocab(self, corpus, special_tokens=[]):\n",
        "        output = {}\n",
        "        i = 0\n",
        "        for st in special_tokens:\n",
        "            output[st] = i\n",
        "            i += 1\n",
        "        for sentence in corpus:\n",
        "            for w in sentence.split():\n",
        "                if w not in output:\n",
        "                    output[w] = i\n",
        "                    i += 1\n",
        "        return output"
      ],
      "metadata": {
        "id": "DGUP_Es0w8NF"
      },
      "id": "DGUP_Es0w8NF",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
      ],
      "metadata": {
        "id": "izzf7p6iw-jk"
      },
      "id": "izzf7p6iw-jk",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "class PennTreeBank (data.Dataset):\n",
        "    # Mandatory methods are __init__, __len__ and __getitem__\n",
        "    def __init__(self, corpus, lang):\n",
        "        self.source = []\n",
        "        self.target = []\n",
        "\n",
        "        for sentence in corpus:\n",
        "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
        "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
        "            # See example in section 6.2\n",
        "\n",
        "        self.source_ids = self.mapping_seq(self.source, lang)\n",
        "        self.target_ids = self.mapping_seq(self.target, lang)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src= torch.LongTensor(self.source_ids[idx])\n",
        "        trg = torch.LongTensor(self.target_ids[idx])\n",
        "        sample = {'source': src, 'target': trg}\n",
        "        return sample\n",
        "\n",
        "    # Auxiliary methods\n",
        "\n",
        "    def mapping_seq(self, data, lang): # Map sequences of tokens to corresponding computed in Lang class\n",
        "        res = []\n",
        "        for seq in data:\n",
        "            tmp_seq = []\n",
        "            for x in seq:\n",
        "                if x in lang.word2id:\n",
        "                    tmp_seq.append(lang.word2id[x])\n",
        "                else:\n",
        "                    print('OOV found!')\n",
        "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
        "                    break\n",
        "            res.append(tmp_seq)\n",
        "        return res"
      ],
      "metadata": {
        "id": "OSzFJN3VxB-g"
      },
      "id": "OSzFJN3VxB-g",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PennTreeBank(train_raw, lang)\n",
        "dev_dataset = PennTreeBank(dev_raw, lang)\n",
        "test_dataset = PennTreeBank(test_raw, lang)"
      ],
      "metadata": {
        "id": "gxs9dq8UxEiR"
      },
      "id": "gxs9dq8UxEiR",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(data, pad_token):\n",
        "    def merge(sequences):\n",
        "        '''\n",
        "        merge from batch * sent_len to batch * max_len\n",
        "        '''\n",
        "        lengths = [len(seq) for seq in sequences]\n",
        "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
        "        # Pad token is zero in our case\n",
        "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape\n",
        "        # batch_size X maximum length of a sequence\n",
        "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
        "        for i, seq in enumerate(sequences):\n",
        "            end = lengths[i]\n",
        "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
        "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
        "        return padded_seqs, lengths\n",
        "\n",
        "    # Sort data by seq lengths\n",
        "\n",
        "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True)\n",
        "    new_item = {}\n",
        "    for key in data[0].keys():\n",
        "        new_item[key] = [d[key] for d in data]\n",
        "\n",
        "    source, _ = merge(new_item[\"source\"])\n",
        "    target, lengths = merge(new_item[\"target\"])\n",
        "\n",
        "    new_item[\"source\"] = source.to(DEVICE)\n",
        "    new_item[\"target\"] = target.to(DEVICE)\n",
        "    new_item[\"number_tokens\"] = sum(lengths)\n",
        "    return new_item\n",
        "\n",
        "# Dataloader instantiation\n",
        "# You can reduce the batch_size if the GPU memory is not enough\n",
        "# take inputs from our data, we have to define the way\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=128, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
      ],
      "metadata": {
        "id": "QHWwNN29xJDt"
      },
      "id": "QHWwNN29xJDt",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def train_loop(data, optimizer, criterion, model, clip=5):\n",
        "    model.train()\n",
        "    loss_array = []\n",
        "    number_of_tokens = []\n",
        "\n",
        "    for sample in data:\n",
        "        optimizer.zero_grad() # Zeroing the gradient\n",
        "        output = model(sample['source'])\n",
        "        loss = criterion(output, sample['target'])\n",
        "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
        "        number_of_tokens.append(sample[\"number_tokens\"])\n",
        "        loss.backward() # Compute the gradient, deleting the computational graph\n",
        "        # clip the gradient to avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step() # Update the weights\n",
        "\n",
        "    return sum(loss_array)/sum(number_of_tokens)\n",
        "\n",
        "def eval_loop(data, eval_criterion, model):\n",
        "    model.eval()\n",
        "    loss_to_return = []\n",
        "    loss_array = []\n",
        "    number_of_tokens = []\n",
        "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
        "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
        "        for sample in data:\n",
        "            output = model(sample['source'])\n",
        "            loss = eval_criterion(output, sample['target'])\n",
        "            loss_array.append(loss.item())\n",
        "            number_of_tokens.append(sample[\"number_tokens\"])\n",
        "\n",
        "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
        "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
        "    return ppl, loss_to_return\n",
        "\n",
        "def init_weights(mat):\n",
        "    for m in mat.modules():\n",
        "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight_ih' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'weight_hh' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'bias' in name:\n",
        "                    param.data.fill_(0)\n",
        "        else:\n",
        "            if type(m) in [nn.Linear]:\n",
        "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
        "                if m.bias != None:\n",
        "                    m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "i3faWQhxxMgD"
      },
      "id": "i3faWQhxxMgD",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# Experiment also with a smaller or bigger model by changing hid and emb sizes\n",
        "# A large model tends to overfit\n",
        "hid_size = 256\n",
        "emb_size = 200\n",
        "\n",
        "# Don't forget to experiment with a lower training batch size\n",
        "# Increasing the back propagation steps can be seen as a regularization step\n",
        "\n",
        "# With SGD try with an higher learning rate (> 1 for instance)\n",
        "lr = 0.0001 # This is definitely not good for SGD\n",
        "clip = 5 # Clip the gradient\n",
        "\n",
        "vocab_len = len(lang.word2id)\n",
        "\n",
        "model = LM_LSTM(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(DEVICE)\n",
        "model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
        "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
      ],
      "metadata": {
        "id": "y9eXbrNuxNj5"
      },
      "id": "y9eXbrNuxNj5",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_API_KEY\"] = \"e968bafef717b24b700b83c82b6660ced56c9315\"  # Replace with your actual key"
      ],
      "metadata": {
        "id": "XQxE-nDexL7A"
      },
      "id": "XQxE-nDexL7A",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"NLU\",\n",
        "    name=\"LSTM-ADAMW-lr=.0001\",\n",
        "    config={\n",
        "        \"epochs\": 100,\n",
        "        \"patience\": 3,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"batch_size\": train_loader.batch_size,\n",
        "        # Add any other parameters you want to track\n",
        "    },\n",
        "    notes=\"LSTM with lr=0.01, hid_size=400, emb_size=200 with ADAMw\"\n",
        ")"
      ],
      "metadata": {
        "id": "PBQy_olYbcCl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "275dbdc1-86d3-4863-96b2-aad41e52cd2e"
      },
      "id": "PBQy_olYbcCl",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250627_153305-e9bzq5vj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/razvanstefan-manole-university-of-trento/NLU/runs/e9bzq5vj' target=\"_blank\">LSTM-ADAMW-lr=.0001</a></strong> to <a href='https://wandb.ai/razvanstefan-manole-university-of-trento/NLU' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/razvanstefan-manole-university-of-trento/NLU' target=\"_blank\">https://wandb.ai/razvanstefan-manole-university-of-trento/NLU</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/razvanstefan-manole-university-of-trento/NLU/runs/e9bzq5vj' target=\"_blank\">https://wandb.ai/razvanstefan-manole-university-of-trento/NLU/runs/e9bzq5vj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/razvanstefan-manole-university-of-trento/NLU/runs/e9bzq5vj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f165f06df90>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "n_epochs = 100\n",
        "patience = 5\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "sampled_epochs = []\n",
        "best_ppl = math.inf\n",
        "best_model = None\n",
        "pbar = tqdm(range(1,n_epochs))\n",
        "#If the PPL is too high try to change the learning rate\n",
        "for epoch in pbar:\n",
        "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        sampled_epochs.append(epoch)\n",
        "        avg_train_loss = np.asarray(loss).mean()\n",
        "        wandb.log({\"train_loss\": avg_train_loss}, step=epoch)\n",
        "\n",
        "        sampled_epochs.append(epoch)\n",
        "        losses_train.append(avg_train_loss)\n",
        "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
        "        avg_dev_loss = np.asarray(loss_dev).mean()\n",
        "\n",
        "        losses_dev.append(avg_dev_loss)\n",
        "        wandb.log({\"dev_loss\": avg_dev_loss, \"dev_ppl\": ppl_dev}, step=epoch)\n",
        "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
        "        if  ppl_dev < best_ppl: # the lower, the better\n",
        "            best_ppl = ppl_dev\n",
        "            best_model = copy.deepcopy(model).to('cpu')\n",
        "            wandb.run.summary[\"best_dev_ppl\"] = best_ppl\n",
        "            patience = 5\n",
        "        else:\n",
        "            patience -= 1\n",
        "\n",
        "        if patience <= 0: # Early stopping with patience\n",
        "            print(f\"Stopping early at epoch {epoch}\")\n",
        "            wandb.run.summary[\"early_stopping_epoch\"] = epoch\n",
        "            break # Not nice but it keeps the code clean\n",
        "\n",
        "best_model.to(DEVICE)\n",
        "final_ppl,  final_loss = eval_loop(test_loader, criterion_eval, best_model)\n",
        "print('Test ppl: ', final_ppl)\n",
        "wandb.run.summary[\"test_ppl\"] = final_ppl\n",
        "wandb.run.summary[\"final_test_loss\"] = np.asarray(final_loss).mean()\n",
        "\n",
        "torch.save(best_model.state_dict(), \"best_model.pth\")\n",
        "wandb.save(\"best_model.pth\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "W_8y07jIxTcH",
        "outputId": "9fe879d8-91fa-4de3-97b9-6811796ece0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "id": "W_8y07jIxTcH",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PPL: 143.486058:  93%|█████████▎| 92/99 [32:31<02:28, 21.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping early at epoch 93\n",
            "Test ppl:  130.2530891338333\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dev_loss</td><td>█▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>dev_ppl</td><td>█▄▄▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_dev_ppl</td><td>143.29021</td></tr><tr><td>dev_loss</td><td>4.96624</td></tr><tr><td>dev_ppl</td><td>143.48606</td></tr><tr><td>early_stopping_epoch</td><td>93</td></tr><tr><td>final_test_loss</td><td>4.86948</td></tr><tr><td>test_ppl</td><td>130.25309</td></tr><tr><td>train_loss</td><td>4.1567</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">LSTM-ADAMW-lr=.0001</strong> at: <a href='https://wandb.ai/razvanstefan-manole-university-of-trento/NLU/runs/e9bzq5vj' target=\"_blank\">https://wandb.ai/razvanstefan-manole-university-of-trento/NLU/runs/e9bzq5vj</a><br> View project at: <a href='https://wandb.ai/razvanstefan-manole-university-of-trento/NLU' target=\"_blank\">https://wandb.ai/razvanstefan-manole-university-of-trento/NLU</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250627_153305-e9bzq5vj/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To save the model\n",
        "path = '/content/drive/MyDrive/NLU/model_bin/LSTM_ADAMW_lr_0.01.pt'\n",
        "torch.save(model.state_dict(), path)\n",
        "#To load the model you need to initialize it\n",
        "model = LM_LSTM(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(DEVICE)\n",
        "#Then you load it\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "rs7Sw1C4D5bt",
        "outputId": "c0ff2eaf-036e-4657-fa98-96b890643066",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rs7Sw1C4D5bt",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}